
{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "import nltk\n",
      "import numpy as np\n",
      "from nltk.stem.porter import *\n",
      "from nltk.corpus import stopwords\n",
      "import sklearn\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from nltk.metrics import windowdiff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function to plot graphs required for Text Tiling Algorithm\n",
      "def plot_fig(x,score,heading,fig_no):\n",
      "    fig = plt.figure(fig_no,figsize=(10,6))\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.plot(x,score,label=heading)\n",
      "    ax.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Open file in python\n",
      "\n",
      "f = open('alien-life.txt','r')\n",
      "doc=f.read()\n",
      "f.close()\n",
      "# The em-dash(long hyphen) is not recognised by ASCII. So, it is replaced with the hyphen.\n",
      "if \"\u2014\" in doc:\n",
      "    doc = doc.replace(\"\u2014\", \"-\")\n",
      "#The original encoding of the text-file is in utf-8 format.So decoding the string from utf-8\n",
      "doc=doc.decode('utf-8')\n",
      "#Encoding it into ascii format\n",
      "doc=doc.encode('ascii','ignore')\n",
      "doc=doc.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Saving a copy of original corpus along with the $$ sign to be later used for Windowdiff measure\n",
      "doc_copy=doc\n",
      "ref_word=[]\n",
      "p = re.compile(r'(\\n)|(\\r)|(\\t)|([!\"#%&()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~])', re.IGNORECASE)\n",
      "doc_copy=re.sub(p,' ',doc_copy)\n",
      "stemmer=PorterStemmer()\n",
      "for word in doc_copy.split(\" \"):\n",
      "    if word not in stopwords.words('english') and stemmer.stem_word(word) not in stopwords.words('english'):\n",
      "        if word!=\" \" and word!=\"\":\n",
      "            ref_word.append(stemmer.stem_word(word))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "TextTiling Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Preparation for TextTiling Algorithm Implementation\n",
      "#Stripping punctuation\n",
      "#Removing new-line characters and punctuations\n",
      "p = re.compile(r'(\\n)|(\\r)|(\\t)|([!\"#$%&()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~])', re.IGNORECASE)\n",
      "doc=re.sub(p,' ',doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sen_length represents the desired sentence length that we are focusing on \n",
      "#Doc represents the corpus of text\n",
      "#Fig no represents the figure no that we want to give to the graph\n",
      "def text_tiling(doc,sen_len,fig_no):\n",
      "    \n",
      "    #Porter Stemming and removing stop words\n",
      "    stemmer=PorterStemmer() \n",
      "    sentences=[]\n",
      "    sentence=\"\"\n",
      "    j=0\n",
      "    for word in doc.split(\" \"):\n",
      "        if word not in stopwords.words('english') and word!=\" \" and word!=\"\":\n",
      "            j=j+1\n",
      "            sentence=sentence + stemmer.stem_word(word)+' '\n",
      "            if j==sen_len:\n",
      "                # -1 is to prevent the whitespace that is appended at the end to be included in the sentence\n",
      "                sentences.append(sentence[:-1])\n",
      "                sentence=\"\"\n",
      "                j=0\n",
      "    #If the last sentence is of length less than sen_length\n",
      "    sentences.append(sentence[:-1])\n",
      " \n",
      "    #Vectorizing Sentences using Sklearn to determine Cosine Similarity between adjacent sentences\n",
      "    tfidf_vectorizer = TfidfVectorizer()\n",
      "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
      "    score=[]\n",
      "    for i in range(0,tfidf_matrix.shape[0]-2):\n",
      "        score.append(cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix[i+1:i+2])[0][0])\n",
      "    \n",
      "    #Plotting Cosine Similarity \n",
      "    plot_fig(range(1,tfidf_matrix.shape[0]-1),score,'Lexical Similarity with Sentence Length '+ str(sen_len),fig_no)\n",
      "    \n",
      "    #Implementing Windowdiff measure\n",
      "    mean_score=np.mean(score)\n",
      "    std_score=np.std(score)\n",
      "    #Threshold is defined as Mean Score - Standard Deviation\n",
      "    threshold=mean_score-std_score\n",
      "    boundary=[]\n",
      "    for i in range(0,len(score)-2): \n",
      "        #score[0] represents the cosine similarity between sentence 1 and sentence 2, score[1] between 2 and 3 and score[2] between 3 and 4\n",
      "        #If depth is greater than threshold, then there will be a dissimilariy between sentence 2 and sentence 3, so we are marking sentence 2 as the boundary\n",
      "        depth=score[i]-score[i+1]+score[i+2]-score[i+1]\n",
      "        if depth>=threshold:\n",
      "            boundary.append(i+1) #Storing positions of Sentences that represent a boundary\n",
      "    #Replacing boundaries with 1 and words with 0\n",
      "    boundary_string=''\n",
      "    for i in range(0,len(sentences)):\n",
      "        boundary_string=boundary_string+sentences[i]+' '\n",
      "        if i in boundary:\n",
      "            boundary_string=boundary_string+' $$ '  \n",
      "        \n",
      "    return boundary_string\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Windowdiff Measure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def window_diff(ref_word,doc,sen_len,fig_no):\n",
      "    #Call to TextTiling Function\n",
      "    hypo=text_tiling(doc,sen_len,fig_no)\n",
      "    #Splitting Hypothesis Words\n",
      "    hypo_words=[word for word in hypo.split(' ') if word!='' and word not in stopwords.words('english')]\n",
      "    #Storing supervised input\n",
      "    para_words=ref_word\n",
      "    i=0\n",
      "    j=0\n",
      "    hypo_string=''\n",
      "    para_string=''\n",
      "    # What this loop does can be explained by an example: Suppose we have total of 3 words and \n",
      "    # we have boundaries after two words in hypothesis  (hey man $$ yep) and after one word in training( hey $$ man yep)\n",
      "    # so this would represent hypothesis string as 00010(0(hey)0($$ in reference)0(man)1($$ in hypothesis)0(yep)) \n",
      "    # reference string as 01000 (0(hey)1($$ in ref)0(man)0($$ in hypothesis)0(yep))\n",
      "    while i<len(hypo_words) and j<len(para_words):\n",
      "        if hypo_words[i]==para_words[j]:\n",
      "            hypo_string=hypo_string+'0'\n",
      "            para_string=para_string+'0'\n",
      "            i=i+1\n",
      "            j=j+1\n",
      "        elif hypo_words[i]=='$$':\n",
      "            hypo_string=hypo_string+'1'\n",
      "            para_string=para_string+'0'\n",
      "            i=i+1\n",
      "        elif para_words[j]=='$$':\n",
      "            hypo_string=hypo_string+'0'\n",
      "            para_string=para_string+'1'\n",
      "            j=j+1\n",
      "    \n",
      "    #Small windows produce more negatives, thus WindowDiff recommends using a window size (k) of half the average segment length.\n",
      "    #Referred from this paper: Getting More from Segmentation Evaluation\n",
      "    \n",
      "    k=sen_len/2\n",